# Probabilistic Pathfinding with Monte Carlo â€“ Soft Policy

This project models a pathfinding agent that moves through a one-dimensional grid from `START` to one of two terminal states (`T1` or `T2`) using a soft-policy Monte Carlo approach.

---

## ğŸ“Œ Problem Description

- The environment is linear, consisting of 8 states: `T1`, `A`, `START`, `B`, `C`, `D`, `E`, `T2`
- The agent starts at `START` and chooses between `left` and `right` actions
- All transitions are **deterministic**, except one:  
  From state `A`, taking action `left` leads to `T1` with 0.2 probability and to `B` with 0.8
- Rewards:
  - All transitions: -1
  - Terminal states (`T1`, `T2`): 0

---

## ğŸ” Methodology

- Algorithm: **First-Visit Monte Carlo**  
- Policy type: **Soft-policy** with Îµ = 0.4  
- Action selection is probabilistic based on Îµ-greedy soft strategy
- State-action values `Q(s,a)` are estimated from returns
- Policy is improved iteratively from estimated Q-values
- Three values of Î³ were tested: **1.0, 0.5, 0.1**

---

## ğŸ“Š Output

- For each Î³ value, the learned policy is printed in terminal
- Observations:
  - Î³ = 1.0 â†’ favors long-term reward, prefers `T2`
  - Î³ = 0.1 â†’ short-sighted, prefers `T1` due to fewer steps
  - Î³ = 0.5 â†’ mixed behavior

---

## ğŸ“ File Structure

- `main.py`: Full Monte Carlo implementation with soft policy exploration

---

## âš ï¸ Notes

- No external libraries were used (NumPy only)
- This implementation does not use any visualization; all results are shown in console
- The project was developed for academic exploration and experimentation
